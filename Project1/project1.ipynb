{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_input: torch.Size([1000, 2, 14, 14])\n",
      "Size of train_target: torch.Size([1000])\n",
      "Size of train_classes: torch.Size([1000, 2])\n",
      "Size of test_input: torch.Size([1000, 2, 14, 14])\n",
      "Size of test_target: torch.Size([1000])\n",
      "Size of test_classes: torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "# We import 1000 pairs of digits for the training and the testing inputs, targets and classes\n",
    "number_of_pairs = 1000 \n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets( \n",
    "    number_of_pairs)\n",
    "\n",
    "# Quick verification of the sizes:\n",
    "print('Size of train_input:', train_input.size())\n",
    "print('Size of train_target:', test_target.size())\n",
    "print('Size of train_classes:', test_classes.size())\n",
    "print('Size of test_input:', test_input.size())\n",
    "print('Size of test_target:', test_target.size())\n",
    "print('Size of test_classes:', test_classes.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, initial_layers=2, final_layers=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(initial_layers, 32, kernel_size=3)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(200)\n",
    "        self.fc2 = nn.Linear(200, final_layers)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(final_layers)\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2)))\n",
    "        x = self.batchnorm2(F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2)))\n",
    "        x = self.batchnorm3(F.relu(self.fc1(x.view(-1, 256))))\n",
    "        x = self.batchnorm4(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Weight Sharing\n",
    "\n",
    "#### We define the same class wether we use auxiliary loss or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSharing(nn.Module):\n",
    "    def __init__(self, auxiliary_loss = False):\n",
    "        super().__init__()\n",
    "        self.sharedConvNet = ConvNet(initial_layers=1, final_layers=10)\n",
    "        self.auxiliary_loss = auxiliary_loss\n",
    "\n",
    "        self.fc1 = nn.Linear(20,100)\n",
    "        self.fc2 = nn.Linear(100,20)\n",
    "        self.fc3 = nn.Linear(20,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        image1 = x.narrow(1,0,1)\n",
    "        image2 = x.narrow(1,1,1)\n",
    "\n",
    "        output1 = self.sharedConvNet(image1)\n",
    "        output2 = self.sharedConvNet(image2)\n",
    "        outputCat = torch.cat((output1, output2), 1)\n",
    "\n",
    "        outputCat = F.relu(self.fc1(outputCat))\n",
    "        outputCat = F.relu(self.fc2(outputCat))\n",
    "        outputCat = self.fc3(outputCat)\n",
    "\n",
    "        \n",
    "        if self.auxiliary_loss:\n",
    "            return outputCat, output1, output2\n",
    "        else:\n",
    "            return outputCat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and number of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size, nb_epochs = 25, lr = 5e-1, printAccLoss = False, auxiliary_loss = False, alpha = 0.3, beta = 0.5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            if auxiliary_loss:\n",
    "                trainTargetImage1 = (train_classes.narrow(1,0,1)).squeeze()\n",
    "                trainTargetImage2 = (train_classes.narrow(1,1,1)).squeeze()\n",
    "\n",
    "                outputCat, output1, output2 = model(train_input.narrow(0, b, mini_batch_size))\n",
    "                lossCat = criterion(outputCat, train_target.narrow(0, b, mini_batch_size))\n",
    "                loss1 = criterion(output1, trainTargetImage1.narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(output2, trainTargetImage2.narrow(0, b, mini_batch_size))\n",
    "\n",
    "                loss = alpha*loss1 + alpha*loss2 + beta*lossCat\n",
    "\n",
    "            else:\n",
    "                output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "                loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        if printAccLoss:\n",
    "            print(e, acc_loss)\n",
    "\n",
    "def compute_nb_errors(model, input, target, mini_batch_size, auxiliary_loss = False):\n",
    "    nb_errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        if auxiliary_loss:\n",
    "            outputCat, _, _ = model(input.narrow(0, b, mini_batch_size))\n",
    "            _, predicted_classes = outputCat.max(1)\n",
    "            for k in range(mini_batch_size):\n",
    "                if target[b + k] != predicted_classes[k]:\n",
    "                    nb_errors = nb_errors + 1\n",
    "        else:\n",
    "            output = model(input.narrow(0, b, mini_batch_size))\n",
    "            _, predicted_classes = output.max(1)\n",
    "            for k in range(mini_batch_size):\n",
    "                if target[b + k] != predicted_classes[k]:\n",
    "                    nb_errors = nb_errors + 1\n",
    "\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "mini_batch_size = 50\n",
    "number_of_runs = 10\n",
    "\n",
    "def test_model(model_name, number_of_runs=10, print_times=False, print_accuracy=False, auxiliary_loss=False):\n",
    "    train_error_ratios = []\n",
    "    test_error_ratios = []\n",
    "    for run in range(number_of_runs):\n",
    "        if model_name == 'ConvNet':\n",
    "            model = ConvNet()\n",
    "        elif model_name == 'WeightSharing':\n",
    "            model = WeightSharing()\n",
    "        elif model_name == 'WeightSharingWithAuxiliaryLoss':\n",
    "            model = WeightSharing(auxiliary_loss=True)\n",
    "        else:\n",
    "            raise ValueError('Please use one of the implemented methods: ConvNet, WeightSharing, WeightSharingWithAuxiliaryLoss')\n",
    "            \n",
    "        train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(number_of_pairs)\n",
    "\n",
    "        if print_times: startTime = time.time()\n",
    "        model.train(True)\n",
    "        train_model(model, train_input, train_target, mini_batch_size, auxiliary_loss=auxiliary_loss)\n",
    "        model.train(False)\n",
    "\n",
    "        if print_times: endTime = time.time()\n",
    "\n",
    "        nb_train_errors = compute_nb_errors(model, train_input, train_target, mini_batch_size,auxiliary_loss=auxiliary_loss)\n",
    "        nb_test_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size, auxiliary_loss=auxiliary_loss)\n",
    "        train_error_ratios.append(nb_train_errors/train_input.size(0))\n",
    "        test_error_ratios.append(nb_test_errors/test_input.size(0))\n",
    "\n",
    "        if print_accuracy:\n",
    "            print('--- Run', run, '---')\n",
    "            print('- train error ratio:', nb_train_errors/train_input.size(0))\n",
    "            print('- test error ratio:', nb_test_errors/test_input.size(0))\n",
    "        if print_times: print('- time for training:', str(endTime-startTime))\n",
    "    return train_error_ratios, test_error_ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Run 0 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.173\n",
      "--- Run 1 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.171\n",
      "--- Run 2 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.184\n",
      "--- Run 3 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.171\n",
      "--- Run 4 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.19\n",
      "--- Run 5 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.178\n",
      "--- Run 6 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.161\n",
      "--- Run 7 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.187\n",
      "--- Run 8 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.187\n",
      "--- Run 9 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.182\n"
     ]
    }
   ],
   "source": [
    "train_error_ratios, test_error_ratios = test_model('ConvNet', print_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Run 0 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.115\n",
      "--- Run 1 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.141\n",
      "--- Run 2 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.113\n",
      "--- Run 3 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.113\n",
      "--- Run 4 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.137\n",
      "--- Run 5 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.122\n",
      "--- Run 6 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.117\n",
      "--- Run 7 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.101\n",
      "--- Run 8 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.107\n",
      "--- Run 9 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.102\n"
     ]
    }
   ],
   "source": [
    "train_error_ratios, test_error_ratios = test_model('WeightSharing', print_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Weight Sharing and Auxiliary Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Run 0 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.195\n",
      "--- Run 1 ---\n",
      "- train error ratio: 0.006\n",
      "- test error ratio: 0.189\n",
      "--- Run 2 ---\n",
      "- train error ratio: 0.017\n",
      "- test error ratio: 0.22\n",
      "--- Run 3 ---\n",
      "- train error ratio: 0.032\n",
      "- test error ratio: 0.226\n",
      "--- Run 4 ---\n",
      "- train error ratio: 0.016\n",
      "- test error ratio: 0.224\n",
      "--- Run 5 ---\n",
      "- train error ratio: 0.017\n",
      "- test error ratio: 0.212\n",
      "--- Run 6 ---\n",
      "- train error ratio: 0.005\n",
      "- test error ratio: 0.171\n",
      "--- Run 7 ---\n",
      "- train error ratio: 0.013\n",
      "- test error ratio: 0.233\n",
      "--- Run 8 ---\n",
      "- train error ratio: 0.003\n",
      "- test error ratio: 0.213\n",
      "--- Run 9 ---\n",
      "- train error ratio: 0.029\n",
      "- test error ratio: 0.209\n"
     ]
    }
   ],
   "source": [
    "train_error_ratios, test_error_ratios = test_model('WeightSharingWithAuxiliaryLoss', auxiliary_loss=True, print_accuracy=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e885cd74143ad494932267455ab53278514454996393c47fe6c2589217b9edf3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
