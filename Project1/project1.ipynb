{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "\n",
    "#### By Arnaud Savary and Jeremie Guy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_input: torch.Size([1000, 2, 14, 14])\n",
      "Size of train_target: torch.Size([1000])\n",
      "Size of train_classes: torch.Size([1000, 2])\n",
      "Size of test_input: torch.Size([1000, 2, 14, 14])\n",
      "Size of test_target: torch.Size([1000])\n",
      "Size of test_classes: torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "# We import 1000 pairs of digits for the training and the testing inputs, targets and classes\n",
    "number_of_pairs = 1000 \n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets( \n",
    "    number_of_pairs)\n",
    "\n",
    "# Quick verification of the sizes:\n",
    "print('Size of train_input:', train_input.size())\n",
    "print('Size of train_target:', train_target.size())\n",
    "print('Size of train_classes:', train_classes.size())\n",
    "print('Size of test_input:', test_input.size())\n",
    "print('Size of test_target:', test_target.size())\n",
    "print('Size of test_classes:', test_classes.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a convolutional network that take batches of 2 images as input\n",
    "# The input therfore is N * 2 * 14 * 14 \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, initial_layers=2, final_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # We define here the layers of the network\n",
    "        self.conv1 = nn.Conv2d(initial_layers, 32, kernel_size=3)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.linear1 = nn.Linear(256, 200)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(200)\n",
    "        self.linear2 = nn.Linear(200, final_layers)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(final_layers)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward step of the network, we entwine activation functions with the previous layers and apply \n",
    "        # batch normalization and dropout\n",
    "        x = self.dropout1(self.batchnorm1(F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))))\n",
    "        x = self.dropout1(self.batchnorm2(F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))))\n",
    "        x = self.dropout2(self.batchnorm3(F.relu(self.linear1(x.view(-1, 256)))))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Weight Sharing\n",
    "\n",
    "##### We create a unique class for weight sharing, wether auxiliary loss is used or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a convolutional network that take batches of 2 images as input\n",
    "# The input therfore is N * 2 * 14 * 14 \n",
    "# It uses the previously defined CNN as a base and implement the weight\n",
    "# sharing mechanism. It can output auxiliary losses if the corresponding \n",
    "# boolean is set to True.\n",
    "class WeightSharing(nn.Module):\n",
    "    def __init__(self, auxiliary_loss = False):\n",
    "        super().__init__()\n",
    "        # We need to use the previously defined CNN but with only 1 node\n",
    "        # for the initialization layer and 10 nodes for the final layer\n",
    "        # Those 10 final layers will then be merged and we will apply a \n",
    "        # few additional layers before making the final prediction\n",
    "        self.sharedConvNet = ConvNet(initial_layers=1, final_layers=10)\n",
    "        self.auxiliary_loss = auxiliary_loss\n",
    "\n",
    "        # Those layers are here for the final prediction, once the 2 CNN\n",
    "        # have been merged\n",
    "        self.linear1 = nn.Linear(20,100)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(100)\n",
    "        self.linear2 = nn.Linear(100,20)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(20)\n",
    "        self.linear3 = nn.Linear(20,2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # We split the input in 2 separate set of images\n",
    "        image1 = x.narrow(1,0,1)\n",
    "        image2 = x.narrow(1,1,1)\n",
    "\n",
    "        # We call the CNN on thoses sets and merge the results\n",
    "        output1 = self.sharedConvNet(image1)\n",
    "        output2 = self.sharedConvNet(image2)\n",
    "        outputCat = torch.cat((output1, output2), 1)\n",
    "\n",
    "        # We apply our previously defined layers, as well as ReLu, batch \n",
    "        # normalization and dropout\n",
    "        outputCat = self.dropout(self.batchnorm1(F.relu(self.linear1(outputCat))))\n",
    "        outputCat = self.dropout(self.batchnorm2(F.relu(self.linear2(outputCat))))\n",
    "        outputCat = F.relu(self.linear3(outputCat))\n",
    "\n",
    "        # If we want to use auxiliary losses we return the outputs of the 2 CNN\n",
    "        if self.auxiliary_loss:\n",
    "            return outputCat, output1, output2\n",
    "        else:\n",
    "            return outputCat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and number of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function trains a given model with the given datasets, mini_batch_size, lr, alpha and gamma.\n",
    "# It prints the accumulated loss if asked to do so and returns it.\n",
    "def train_model(model, train_input, train_target, train_classes, mini_batch_size, nb_epochs=50, lr = 5e-1,  alpha = 0.3, gamma = 0.5, auxiliary_loss = False, printAccLoss = False):\n",
    "    losses = []\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "\n",
    "        # Loops on the mini batches\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            # If we want to use auxiliary loss, we compute the losses of the merged and separated data\n",
    "            # and combine them with a weighted sum\n",
    "            if auxiliary_loss:\n",
    "                # Extracts the targets of the separated images\n",
    "                trainTargetImage1 = (train_classes.narrow(1,0,1)).squeeze()\n",
    "                trainTargetImage2 = (train_classes.narrow(1,1,1)).squeeze()\n",
    "\n",
    "                outputCat, output1, output2 = model(train_input.narrow(0, b, mini_batch_size))\n",
    "\n",
    "                mainLoss = criterion(outputCat, train_target.narrow(0, b, mini_batch_size))\n",
    "                auxiliaryLoss1 = criterion(output1, trainTargetImage1.narrow(0, b, mini_batch_size))\n",
    "                auxiliaryLoss2 = criterion(output2, trainTargetImage2.narrow(0, b, mini_batch_size))\n",
    "\n",
    "                loss = alpha*auxiliaryLoss1 + alpha*auxiliaryLoss2 + gamma*mainLoss\n",
    "\n",
    "            else:\n",
    "                output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "                loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "\n",
    "            # We stores the accumulated loss for future plots\n",
    "            acc_loss += loss.item()\n",
    "            losses.append(acc_loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if printAccLoss:\n",
    "            print(e, acc_loss)\n",
    "\n",
    "    return losses\n",
    "\n",
    "# This function returns the number of errors made by the given model on the given dataset and mini_batch_size\n",
    "def compute_nb_errors(model, input, target, classes, mini_batch_size, auxiliary_loss = False):\n",
    "    nb_errors = 0\n",
    "\n",
    "    # We plit the data in mini-batch\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        # While we don't need to compute loss here, we separate the auxiliary loss model because we need to\n",
    "        # get rid of the unwanted returns\n",
    "        if auxiliary_loss:\n",
    "            output, _, _ = model(input.narrow(0, b, mini_batch_size))\n",
    "        else:\n",
    "            output = model(input.narrow(0, b, mini_batch_size))\n",
    "\n",
    "        # We extract the predicted class and compare it to thr target to compute the number of errors    \n",
    "        _, predicted_classes = output.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if target[b + k] != predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import split\n",
    "\n",
    "# This function returns the best paramaters for the given model name on an average of 5 rounds with the methode named grid search\n",
    "def tune_model(model_name, number_of_rounds=5):\n",
    "    auxiliary_loss=False\n",
    "    mini_batch_size = 50\n",
    "    \n",
    "    # Diferent values for each parameter\n",
    "    lrs = [5e-2, 1e-1, 5e-1, 1]\n",
    "    alphas = [0.2, 0.5, 0.8, 1]\n",
    "    gammas = [0.2, 0.5, 0.8, 1]\n",
    "\n",
    "    # Stores the best parameter\n",
    "    bestLr = 0\n",
    "    bestAlpha = 0\n",
    "    bestGamma = 0\n",
    "    best_percentage_validation_errors = 1\n",
    "\n",
    "    # We loop on every combination of parameters\n",
    "    for lr in lrs:\n",
    "        if model_name == 'WeightSharingWithAuxiliaryLoss':\n",
    "            for alpha in alphas:\n",
    "                for gamma in gammas:\n",
    "                    percentage_validation_errors = []\n",
    "                    for _ in range(number_of_rounds):\n",
    "                        auxiliary_loss=True\n",
    "\n",
    "                        # We generate a new model and dataset\n",
    "                        model = WeightSharing(auxiliary_loss)\n",
    "                        input, target, classes, _, _, _ = prologue.generate_pair_sets(number_of_pairs)\n",
    "                        \n",
    "                        # We split the datasets in training and validation input, target, class\n",
    "                        train_input, validation_input = split(input, [int(number_of_pairs*0.5), int(number_of_pairs*0.5)])\n",
    "                        train_target, validation_target = split(target, [int(number_of_pairs*0.5), int(number_of_pairs*0.5)])\n",
    "                        train_classes, validation_classes = split(classes, [int(number_of_pairs*0.5), int(number_of_pairs*0.5)])\n",
    "                        \n",
    "                        # We train the model\n",
    "                        model.train(True)\n",
    "                        train_model(model, train_input, train_target, train_classes, mini_batch_size, nb_epochs=25, lr = lr, alpha = alpha, gamma = gamma, auxiliary_loss=auxiliary_loss)\n",
    "\n",
    "                        # We compute the number of errors and append it to a list\n",
    "                        model.train(False)\n",
    "                        nb_validation_errors = compute_nb_errors(model, validation_input, validation_target, validation_classes, mini_batch_size, auxiliary_loss=auxiliary_loss)\n",
    "                        percentage_validation_errors.append(nb_validation_errors/len(validation_input))\n",
    "\n",
    "                    # If the mean of the number of errors on the 5 runs is better than what we had, we store the current parameters\n",
    "                    if np.mean(percentage_validation_errors) < best_percentage_validation_errors:\n",
    "                        best_percentage_validation_errors = np.mean(percentage_validation_errors)\n",
    "                        bestLr = lr\n",
    "                        bestAlpha = alpha\n",
    "                        bestGamma = gamma\n",
    "                        \n",
    "        else:\n",
    "            percentage_validation_errors = []\n",
    "            for _ in range(number_of_rounds):\n",
    "                # We generate a new model and dataset\n",
    "                if model_name == 'ConvNet':\n",
    "                    model = ConvNet()\n",
    "                elif model_name == 'WeightSharing':\n",
    "                    model = WeightSharing()\n",
    "                else:\n",
    "                    raise ValueError('Please use one of the implemented methods: ConvNet, WeightSharing, WeightSharingWithAuxiliaryLoss')\n",
    "                input, target, classes, _, _, _ = prologue.generate_pair_sets(number_of_pairs)\n",
    "\n",
    "                # We split the datasets in training and validation input, target, class\n",
    "                train_input, validation_input = split(input, [int(number_of_pairs*0.5), int(number_of_pairs*0.5)])\n",
    "                train_target, validation_target = split(target, [int(number_of_pairs*0.5), int(number_of_pairs*0.5)])\n",
    "                train_classes, validation_classes = split(classes, [int(number_of_pairs*0.5), int(number_of_pairs*0.5)])\n",
    "\n",
    "                # We train the model\n",
    "                model.train(True)\n",
    "                train_model(model, train_input, train_target, train_classes, mini_batch_size)\n",
    "\n",
    "                # We compute the number of errors and append it to a list\n",
    "                model.train(False)\n",
    "                nb_validation_errors = compute_nb_errors(model, validation_input, validation_target, validation_classes, mini_batch_size)\n",
    "                percentage_validation_errors.append(nb_validation_errors/len(validation_input))\n",
    "            \n",
    "            # If the mean of the number of errors on the 5 runs is better than what we had, we store the current parameters\n",
    "            if np.mean(percentage_validation_errors) < best_percentage_validation_errors:\n",
    "                best_percentage_validation_errors = np.mean(percentage_validation_errors)\n",
    "                bestLr = lr\n",
    "\n",
    "    print('--- Best parameters ---')\n",
    "    print('lr:', bestLr, ', alpha:', bestAlpha, ', gamma:',bestGamma)\n",
    "    print('Mean percentage error on validation set :', best_percentage_validation_errors)\n",
    "    \n",
    "    return bestLr, bestAlpha, bestGamma\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# This functions runs the model 10 times and prints the mean and std on the number of errors.\n",
    "# It can also print the time for each run according to the corresponding boolean.\n",
    "def test_model(model_name, print_times=False):\n",
    "    train_error_ratios = []\n",
    "    test_error_ratios = []\n",
    "\n",
    "    auxiliary_loss = False\n",
    "    number_of_runs = 15\n",
    "    mini_batch_size = 50\n",
    "\n",
    "    # Extracts the best parameters for the model\n",
    "    bestLr, bestAlpha, bestGamma = tune_model(model_name)\n",
    "\n",
    "    for run in range(number_of_runs):\n",
    "        # For each run we create a new model to allow a random weight initialization\n",
    "        if model_name == 'ConvNet':\n",
    "            model = ConvNet()\n",
    "        elif model_name == 'WeightSharing':\n",
    "            model = WeightSharing()\n",
    "        elif model_name == 'WeightSharingWithAuxiliaryLoss':\n",
    "            auxiliary_loss=True\n",
    "            model = WeightSharing(auxiliary_loss)\n",
    "        else:\n",
    "            raise ValueError('Please use one of the implemented methods: ConvNet, WeightSharing, WeightSharingWithAuxiliaryLoss')\n",
    "        \n",
    "        # We also extract a new random dataset\n",
    "        train_input, train_target, _, test_input, test_target, _ = prologue.generate_pair_sets(number_of_pairs)\n",
    "\n",
    "        # Model training\n",
    "        if print_times: startTime = time.time()\n",
    "        model.train(True)\n",
    "        train_model(model, train_input, train_target, train_classes, mini_batch_size, nb_epochs=50, lr = bestLr,  alpha = bestAlpha, gamma = bestGamma, auxiliary_loss=auxiliary_loss)\n",
    "        model.train(False)\n",
    "\n",
    "        if print_times: endTime = time.time()\n",
    "\n",
    "        # Extracts the number of errors made by the model\n",
    "        nb_train_errors = compute_nb_errors(model, train_input, train_target, train_classes, mini_batch_size, auxiliary_loss=auxiliary_loss)\n",
    "        nb_test_errors = compute_nb_errors(model, test_input, test_target, test_classes, mini_batch_size, auxiliary_loss=auxiliary_loss)\n",
    "\n",
    "        # Divides the number of errors by the length of the dataset to get a percentage\n",
    "        train_error_ratios.append(nb_train_errors/train_input.size(0))\n",
    "        test_error_ratios.append(nb_test_errors/test_input.size(0))\n",
    "\n",
    "        print('--- Run', run, '---')\n",
    "        print('- train error ratio:', nb_train_errors/train_input.size(0))\n",
    "        print('- test error ratio:', nb_test_errors/test_input.size(0))\n",
    "        if print_times: print('- time for training:', str(endTime-startTime))\n",
    "\n",
    "    print('----END----')\n",
    "    print('Train error mean ratio:', np.mean(train_error_ratios))\n",
    "    print(\"Train error ratio's std:\", np.std(train_error_ratios))\n",
    "    print('Test errors mean ratio:', np.mean(test_error_ratios))\n",
    "    print(\"Test error ratio's std:\", np.std(test_error_ratios))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Best parameters ---\n",
      "lr: 0.05 , alpha: 0 , gamma: 0\n",
      "Mean percentage error on validation set : 0.1924\n",
      "--- Run 0 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.158\n",
      "--- Run 1 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.158\n",
      "--- Run 2 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.165\n",
      "--- Run 3 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.147\n",
      "--- Run 4 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.153\n",
      "--- Run 5 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.182\n",
      "--- Run 6 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.168\n",
      "--- Run 7 ---\n",
      "- train error ratio: 0.003\n",
      "- test error ratio: 0.196\n",
      "--- Run 8 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.175\n",
      "--- Run 9 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.184\n",
      "--- Run 10 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.159\n",
      "--- Run 11 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.146\n",
      "--- Run 12 ---\n",
      "- train error ratio: 0.003\n",
      "- test error ratio: 0.2\n",
      "--- Run 13 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.17\n",
      "--- Run 14 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.185\n",
      "----END----\n",
      "Train error mean ratio: 0.0006666666666666668\n",
      "Train error ratio's std: 0.0010110500592068732\n",
      "Test errors mean ratio: 0.16973333333333332\n",
      "Test error ratio's std: 0.01632367061117757\n"
     ]
    }
   ],
   "source": [
    "test_model('ConvNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Best parameters ---\n",
      "lr: 0.1 , alpha: 0 , gamma: 0\n",
      "Mean percentage error on validation set : 0.1512\n",
      "--- Run 0 ---\n",
      "- train error ratio: 0.004\n",
      "- test error ratio: 0.153\n",
      "--- Run 1 ---\n",
      "- train error ratio: 0.003\n",
      "- test error ratio: 0.118\n",
      "--- Run 2 ---\n",
      "- train error ratio: 0.005\n",
      "- test error ratio: 0.113\n",
      "--- Run 3 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.115\n",
      "--- Run 4 ---\n",
      "- train error ratio: 0.006\n",
      "- test error ratio: 0.132\n",
      "--- Run 5 ---\n",
      "- train error ratio: 0.002\n",
      "- test error ratio: 0.107\n",
      "--- Run 6 ---\n",
      "- train error ratio: 0.003\n",
      "- test error ratio: 0.12\n",
      "--- Run 7 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.129\n",
      "--- Run 8 ---\n",
      "- train error ratio: 0.006\n",
      "- test error ratio: 0.137\n",
      "--- Run 9 ---\n",
      "- train error ratio: 0.003\n",
      "- test error ratio: 0.13\n",
      "--- Run 10 ---\n",
      "- train error ratio: 0.002\n",
      "- test error ratio: 0.127\n",
      "--- Run 11 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.114\n",
      "--- Run 12 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.134\n",
      "--- Run 13 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.117\n",
      "--- Run 14 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.099\n",
      "----END----\n",
      "Train error mean ratio: 0.0026000000000000003\n",
      "Train error ratio's std: 0.001854723699099141\n",
      "Test errors mean ratio: 0.123\n",
      "Test error ratio's std: 0.013028174597131147\n"
     ]
    }
   ],
   "source": [
    "test_model('WeightSharing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Weight Sharing and Auxiliary Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Best parameters ---\n",
      "lr: 1 , alpha: 0.2 , gamma: 0.2\n",
      "Mean percentage error on validation set : 0.0596\n",
      "--- Run 0 ---\n",
      "- train error ratio: 0.023\n",
      "- test error ratio: 0.128\n",
      "--- Run 1 ---\n",
      "- train error ratio: 0.007\n",
      "- test error ratio: 0.145\n",
      "--- Run 2 ---\n",
      "- train error ratio: 0.025\n",
      "- test error ratio: 0.166\n",
      "--- Run 3 ---\n",
      "- train error ratio: 0.013\n",
      "- test error ratio: 0.152\n",
      "--- Run 4 ---\n",
      "- train error ratio: 0.018\n",
      "- test error ratio: 0.159\n",
      "--- Run 5 ---\n",
      "- train error ratio: 0.021\n",
      "- test error ratio: 0.15\n",
      "--- Run 6 ---\n",
      "- train error ratio: 0.016\n",
      "- test error ratio: 0.15\n",
      "--- Run 7 ---\n",
      "- train error ratio: 0.01\n",
      "- test error ratio: 0.148\n",
      "--- Run 8 ---\n",
      "- train error ratio: 0.053\n",
      "- test error ratio: 0.154\n",
      "--- Run 9 ---\n",
      "- train error ratio: 0.032\n",
      "- test error ratio: 0.156\n",
      "--- Run 10 ---\n",
      "- train error ratio: 0.011\n",
      "- test error ratio: 0.156\n",
      "--- Run 11 ---\n",
      "- train error ratio: 0.014\n",
      "- test error ratio: 0.164\n",
      "--- Run 12 ---\n",
      "- train error ratio: 0.01\n",
      "- test error ratio: 0.142\n",
      "--- Run 13 ---\n",
      "- train error ratio: 0.011\n",
      "- test error ratio: 0.153\n",
      "--- Run 14 ---\n",
      "- train error ratio: 0.012\n",
      "- test error ratio: 0.158\n",
      "----END----\n",
      "Train error mean ratio: 0.018400000000000003\n",
      "Train error ratio's std: 0.01134195750300626\n",
      "Test errors mean ratio: 0.15206666666666663\n",
      "Test error ratio's std: 0.008984925647376773\n"
     ]
    }
   ],
   "source": [
    "test_model('WeightSharingWithAuxiliaryLoss')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e885cd74143ad494932267455ab53278514454996393c47fe6c2589217b9edf3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
