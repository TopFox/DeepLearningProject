{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_input: torch.Size([1000, 2, 14, 14])\n",
      "Size of train_target: torch.Size([1000])\n",
      "Size of train_classes: torch.Size([1000, 2])\n",
      "Size of test_input: torch.Size([1000, 2, 14, 14])\n",
      "Size of test_target: torch.Size([1000])\n",
      "Size of test_classes: torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "# We import 1000 pairs of digits for the training and the testing inputs, targets and classes\n",
    "number_of_pairs = 1000 \n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets( \n",
    "    number_of_pairs)\n",
    "\n",
    "# Quick verification of the sizes:\n",
    "print('Size of train_input:', train_input.size())\n",
    "print('Size of train_target:', train_target.size())\n",
    "print('Size of train_classes:', train_classes.size())\n",
    "print('Size of test_input:', test_input.size())\n",
    "print('Size of test_target:', test_target.size())\n",
    "print('Size of test_classes:', test_classes.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, initial_layers=2, final_layers=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(initial_layers, 32, kernel_size=3)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(200)\n",
    "        self.fc2 = nn.Linear(200, final_layers)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(final_layers)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.batchnorm1(F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))))\n",
    "        x = self.dropout1(self.batchnorm2(F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))))\n",
    "        x = self.dropout2(self.batchnorm3(F.relu(self.fc1(x.view(-1, 256)))))\n",
    "        x = self.dropout2(self.batchnorm4(self.fc2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Weight Sharing\n",
    "\n",
    "#### We define the same class wether we use auxiliary loss or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSharing(nn.Module):\n",
    "    def __init__(self, auxiliary_loss = False):\n",
    "        super().__init__()\n",
    "        self.sharedConvNet = ConvNet(initial_layers=1, final_layers=10)\n",
    "        self.auxiliary_loss = auxiliary_loss\n",
    "\n",
    "        self.fc1 = nn.Linear(20,100)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(100)\n",
    "        self.fc2 = nn.Linear(100,20)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(20)\n",
    "        self.fc3 = nn.Linear(20,2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        image1 = x.narrow(1,0,1)\n",
    "        image2 = x.narrow(1,1,1)\n",
    "\n",
    "        output1 = self.sharedConvNet(image1)\n",
    "        output2 = self.sharedConvNet(image2)\n",
    "        outputCat = torch.cat((output1, output2), 1)\n",
    "\n",
    "        outputCat = self.dropout(self.batchnorm1(F.relu(self.fc1(outputCat))))\n",
    "        outputCat = self.dropout(self.batchnorm2(F.relu(self.fc2(outputCat))))\n",
    "        outputCat = self.fc3(outputCat)\n",
    "\n",
    "        \n",
    "        if self.auxiliary_loss:\n",
    "            return outputCat, output1, output2\n",
    "        else:\n",
    "            return outputCat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and number of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, train_classes, mini_batch_size, lr = 5e-1,  alpha = 0.3, gamma = 0.5, printAccLoss = False, auxiliary_loss = False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    nb_epochs = 25\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            if auxiliary_loss:\n",
    "                trainTargetImage1 = (train_classes.narrow(1,0,1)).squeeze()\n",
    "                trainTargetImage2 = (train_classes.narrow(1,1,1)).squeeze()\n",
    "\n",
    "                outputCat, output1, output2 = model(train_input.narrow(0, b, mini_batch_size))\n",
    "                lossCat = criterion(outputCat, train_target.narrow(0, b, mini_batch_size))\n",
    "                loss1 = criterion(output1, trainTargetImage1.narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(output2, trainTargetImage2.narrow(0, b, mini_batch_size))\n",
    "\n",
    "                loss = alpha*loss1 + alpha*loss2 + gamma*lossCat\n",
    "\n",
    "            else:\n",
    "                output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "                loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        if printAccLoss:\n",
    "            print(e, acc_loss)\n",
    "\n",
    "def compute_nb_errors(model, input, target, mini_batch_size, auxiliary_loss = False):\n",
    "    nb_errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        if auxiliary_loss:\n",
    "            outputCat, _, _ = model(input.narrow(0, b, mini_batch_size))\n",
    "            _, predicted_classes = outputCat.max(1)\n",
    "            for k in range(mini_batch_size):\n",
    "                if target[b + k] != predicted_classes[k]:\n",
    "                    nb_errors = nb_errors + 1\n",
    "        else:\n",
    "            output = model(input.narrow(0, b, mini_batch_size))\n",
    "            _, predicted_classes = output.max(1)\n",
    "            for k in range(mini_batch_size):\n",
    "                if target[b + k] != predicted_classes[k]:\n",
    "                    nb_errors = nb_errors + 1\n",
    "\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "mini_batch_size = 50\n",
    "\n",
    "def test_model(model_name, print_times=False, print_accuracy=False):\n",
    "    train_error_ratios = []\n",
    "    test_error_ratios = []\n",
    "    auxiliary_loss = False\n",
    "    number_of_runs = 10\n",
    "\n",
    "    for run in range(number_of_runs):\n",
    "        if model_name == 'ConvNet':\n",
    "            model = ConvNet()\n",
    "        elif model_name == 'WeightSharing':\n",
    "            model = WeightSharing()\n",
    "        elif model_name == 'WeightSharingWithAuxiliaryLoss':\n",
    "            auxiliary_loss=True\n",
    "            model = WeightSharing(auxiliary_loss)\n",
    "        else:\n",
    "            raise ValueError('Please use one of the implemented methods: ConvNet, WeightSharing, WeightSharingWithAuxiliaryLoss')\n",
    "            \n",
    "        train_input, train_target, _, test_input, test_target, _ = prologue.generate_pair_sets(number_of_pairs)\n",
    "\n",
    "        if print_times: startTime = time.time()\n",
    "        model.train(True)\n",
    "        train_model(model, train_input, train_target, train_classes, mini_batch_size, auxiliary_loss=auxiliary_loss)\n",
    "        model.train(False)\n",
    "\n",
    "        if print_times: endTime = time.time()\n",
    "\n",
    "        nb_train_errors = compute_nb_errors(model, train_input, train_target, mini_batch_size,auxiliary_loss=auxiliary_loss)\n",
    "        nb_test_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size, auxiliary_loss=auxiliary_loss)\n",
    "        train_error_ratios.append(nb_train_errors/train_input.size(0))\n",
    "        test_error_ratios.append(nb_test_errors/test_input.size(0))\n",
    "\n",
    "        if print_accuracy:\n",
    "            print('--- Run', run, '---')\n",
    "            print('- train error ratio:', nb_train_errors/train_input.size(0))\n",
    "            print('- test error ratio:', nb_test_errors/test_input.size(0))\n",
    "        if print_times: print('- time for training:', str(endTime-startTime))\n",
    "    if print_accuracy: \n",
    "        print('----END----')\n",
    "        print('Train error mean ratio:', np.mean(train_error_ratios))\n",
    "        print('Test errors mean ratio:', np.mean(test_error_ratios))\n",
    "\n",
    "    return train_error_ratios, test_error_ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(model_name):\n",
    "    auxiliary_loss=False\n",
    "    if model_name == 'ConvNet':\n",
    "        model = ConvNet()\n",
    "    elif model_name == 'WeightSharing':\n",
    "        model = WeightSharing()\n",
    "    elif model_name == 'WeightSharingWithAuxiliaryLoss':\n",
    "        auxiliary_loss=True\n",
    "        model = WeightSharing(auxiliary_loss)\n",
    "    else:\n",
    "        raise ValueError('Please use one of the implemented methods: ConvNet, WeightSharing, WeightSharingWithAuxiliaryLoss')\n",
    "    \n",
    "    lrs = []\n",
    "    alphas = []\n",
    "    gammas = []\n",
    "    for lr in lrs:\n",
    "        if model_name == 'WeightSharingWithAuxiliaryLoss':\n",
    "            for alpha in alphas:\n",
    "                for gamma in gammas:\n",
    "                    train_input, train_target, train_classes, _, _, _ = prologue.generate_pair_sets(number_of_pairs)\n",
    "                    model.train(True)\n",
    "                    train_model(model, train_input, train_target, train_classes, mini_batch_size, lr = lr, alpha = alpha, gamma = gamma)\n",
    "                    model.train(False)\n",
    "        else:\n",
    "            train_input, train_target, train_classes, _, _, _ = prologue.generate_pair_sets(number_of_pairs)\n",
    "            model.train(True)\n",
    "            train_model(model, train_input, train_target, train_classes, mini_batch_size, lr = lr)\n",
    "            model.train(False)\n",
    "\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size, auxiliary_loss=auxiliary_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Run 0 ---\n",
      "- train error ratio: 0.007\n",
      "- test error ratio: 0.155\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16016/1948904763.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_error_ratios\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_error_ratios\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ConvNet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16016/2648750900.py\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(model_name, print_times, print_accuracy)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprint_times\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstartTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauxiliary_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauxiliary_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16016/2214253783.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_input, train_target, train_classes, mini_batch_size, lr, alpha, gamma, printAccLoss, auxiliary_loss)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_error_ratios, test_error_ratios = test_model('ConvNet', print_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Run 0 ---\n",
      "- train error ratio: 0.006\n",
      "- test error ratio: 0.116\n",
      "--- Run 1 ---\n",
      "- train error ratio: 0.006\n",
      "- test error ratio: 0.112\n",
      "--- Run 2 ---\n",
      "- train error ratio: 0.011\n",
      "- test error ratio: 0.132\n",
      "--- Run 3 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.122\n",
      "--- Run 4 ---\n",
      "- train error ratio: 0.009\n",
      "- test error ratio: 0.117\n",
      "--- Run 5 ---\n",
      "- train error ratio: 0.002\n",
      "- test error ratio: 0.108\n",
      "--- Run 6 ---\n",
      "- train error ratio: 0.013\n",
      "- test error ratio: 0.131\n",
      "--- Run 7 ---\n",
      "- train error ratio: 0.003\n",
      "- test error ratio: 0.101\n",
      "--- Run 8 ---\n",
      "- train error ratio: 0.013\n",
      "- test error ratio: 0.144\n",
      "--- Run 9 ---\n",
      "- train error ratio: 0.018\n",
      "- test error ratio: 0.12\n",
      "----END----\n",
      "Train error mean ratio: 0.0082\n",
      "Test errors mean ratio: 0.12029999999999999\n"
     ]
    }
   ],
   "source": [
    "train_error_ratios, test_error_ratios = test_model('WeightSharing', print_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Weight Sharing and Auxiliary Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Run 0 ---\n",
      "- train error ratio: 0.011\n",
      "- test error ratio: 0.149\n",
      "--- Run 1 ---\n",
      "- train error ratio: 0.008\n",
      "- test error ratio: 0.129\n",
      "--- Run 2 ---\n",
      "- train error ratio: 0.005\n",
      "- test error ratio: 0.146\n",
      "--- Run 3 ---\n",
      "- train error ratio: 0.006\n",
      "- test error ratio: 0.116\n",
      "--- Run 4 ---\n",
      "- train error ratio: 0.012\n",
      "- test error ratio: 0.144\n",
      "--- Run 5 ---\n",
      "- train error ratio: 0.013\n",
      "- test error ratio: 0.143\n",
      "--- Run 6 ---\n",
      "- train error ratio: 0.007\n",
      "- test error ratio: 0.105\n",
      "--- Run 7 ---\n",
      "- train error ratio: 0.021\n",
      "- test error ratio: 0.131\n",
      "--- Run 8 ---\n",
      "- train error ratio: 0.012\n",
      "- test error ratio: 0.146\n",
      "--- Run 9 ---\n",
      "- train error ratio: 0.007\n",
      "- test error ratio: 0.14\n",
      "----END----\n",
      "Train error mean ratio: 0.0102\n",
      "Test errors mean ratio: 0.13489999999999996\n"
     ]
    }
   ],
   "source": [
    "train_error_ratios, test_error_ratios = test_model('WeightSharingWithAuxiliaryLoss', print_accuracy=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e885cd74143ad494932267455ab53278514454996393c47fe6c2589217b9edf3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
