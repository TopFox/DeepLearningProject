{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_input: torch.Size([1000, 2, 14, 14])\n",
      "Size of train_target: torch.Size([1000])\n",
      "Size of train_classes: torch.Size([1000, 2])\n",
      "Size of test_input: torch.Size([1000, 2, 14, 14])\n",
      "Size of test_target: torch.Size([1000])\n",
      "Size of test_classes: torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "# We import 1000 pairs of digits for the training and the testing inputs, targets and classes\n",
    "number_of_pairs = 1000 \n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets( \n",
    "    number_of_pairs)\n",
    "\n",
    "# Quick verification of the sizes:\n",
    "print('Size of train_input:', train_input.size())\n",
    "print('Size of train_target:', train_target.size())\n",
    "print('Size of train_classes:', train_classes.size())\n",
    "print('Size of test_input:', test_input.size())\n",
    "print('Size of test_target:', test_target.size())\n",
    "print('Size of test_classes:', test_classes.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, initial_layers=2, final_layers=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(initial_layers, 32, kernel_size=3)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(200)\n",
    "        self.fc2 = nn.Linear(200, final_layers)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(final_layers)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.batchnorm1(F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))))\n",
    "        x = self.dropout1(self.batchnorm2(F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))))\n",
    "        x = self.dropout2(self.batchnorm3(F.relu(self.fc1(x.view(-1, 256)))))\n",
    "        x = self.dropout2(self.batchnorm4(self.fc2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Weight Sharing\n",
    "\n",
    "#### We define the same class wether we use auxiliary loss or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSharing(nn.Module):\n",
    "    def __init__(self, auxiliary_loss = False):\n",
    "        super().__init__()\n",
    "        self.sharedConvNet = ConvNet(initial_layers=1, final_layers=10)\n",
    "        self.auxiliary_loss = auxiliary_loss\n",
    "\n",
    "        self.fc1 = nn.Linear(20,100)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(100)\n",
    "        self.fc2 = nn.Linear(100,20)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(20)\n",
    "        self.fc3 = nn.Linear(20,2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        image1 = x.narrow(1,0,1)\n",
    "        image2 = x.narrow(1,1,1)\n",
    "\n",
    "        output1 = self.sharedConvNet(image1)\n",
    "        output2 = self.sharedConvNet(image2)\n",
    "        outputCat = torch.cat((output1, output2), 1)\n",
    "\n",
    "        outputCat = self.dropout(self.batchnorm1(F.relu(self.fc1(outputCat))))\n",
    "        outputCat = self.dropout(self.batchnorm2(F.relu(self.fc2(outputCat))))\n",
    "        outputCat = self.fc3(outputCat)\n",
    "\n",
    "        \n",
    "        if self.auxiliary_loss:\n",
    "            return outputCat, output1, output2\n",
    "        else:\n",
    "            return outputCat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and number of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, train_classes, mini_batch_size, lr = 5e-1,  alpha = 0.3, gamma = 0.5, auxiliary_loss = False, printAccLoss = False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    nb_epochs = 25\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            if auxiliary_loss:\n",
    "                trainTargetImage1 = (train_classes.narrow(1,0,1)).squeeze()\n",
    "                trainTargetImage2 = (train_classes.narrow(1,1,1)).squeeze()\n",
    "\n",
    "                outputCat, output1, output2 = model(train_input.narrow(0, b, mini_batch_size))\n",
    "                lossCat = criterion(outputCat, train_target.narrow(0, b, mini_batch_size))\n",
    "                loss1 = criterion(output1, trainTargetImage1.narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(output2, trainTargetImage2.narrow(0, b, mini_batch_size))\n",
    "\n",
    "                loss = alpha*loss1 + alpha*loss2 + gamma*lossCat\n",
    "\n",
    "            else:\n",
    "                output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "                loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        if printAccLoss:\n",
    "            print(e, acc_loss)\n",
    "\n",
    "def compute_nb_errors(model, input, target, classes, mini_batch_size, auxiliary_loss = False):\n",
    "    nb_errors = 0\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        if auxiliary_loss:\n",
    "            outputCat, _, _ = model(input.narrow(0, b, mini_batch_size))\n",
    "            _, predicted_classes = outputCat.max(1)\n",
    "            for k in range(mini_batch_size):\n",
    "                if target[b + k] != predicted_classes[k]:\n",
    "                    nb_errors = nb_errors + 1\n",
    "        else:\n",
    "            output = model(input.narrow(0, b, mini_batch_size))\n",
    "            _, predicted_classes = output.max(1)\n",
    "            for k in range(mini_batch_size):\n",
    "                if target[b + k] != predicted_classes[k]:\n",
    "                    nb_errors = nb_errors + 1\n",
    "\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "mini_batch_size = 50\n",
    "\n",
    "def test_model(model_name, print_times=False, print_accuracy=False):\n",
    "    train_error_ratios = []\n",
    "    test_error_ratios = []\n",
    "    auxiliary_loss = False\n",
    "    number_of_runs = 10\n",
    "    bestLr, bestAlpha, bestGamma = tune_model(model_name)\n",
    "\n",
    "    for run in range(number_of_runs):\n",
    "        if model_name == 'ConvNet':\n",
    "            model = ConvNet()\n",
    "        elif model_name == 'WeightSharing':\n",
    "            model = WeightSharing()\n",
    "        elif model_name == 'WeightSharingWithAuxiliaryLoss':\n",
    "            auxiliary_loss=True\n",
    "            model = WeightSharing(auxiliary_loss)\n",
    "        else:\n",
    "            raise ValueError('Please use one of the implemented methods: ConvNet, WeightSharing, WeightSharingWithAuxiliaryLoss')\n",
    "            \n",
    "        train_input, train_target, _, test_input, test_target, _ = prologue.generate_pair_sets(number_of_pairs)\n",
    "\n",
    "        if print_times: startTime = time.time()\n",
    "        model.train(True)\n",
    "        train_model(model, train_input, train_target, train_classes, mini_batch_size, lr = bestLr,  alpha = bestAlpha, gamma = bestGamma, auxiliary_loss=auxiliary_loss)\n",
    "        model.train(False)\n",
    "\n",
    "        if print_times: endTime = time.time()\n",
    "\n",
    "        nb_train_errors = compute_nb_errors(model, train_input, train_target, train_classes, mini_batch_size,auxiliary_loss=auxiliary_loss)\n",
    "        nb_test_errors = compute_nb_errors(model, test_input, test_target, test_classes, mini_batch_size, auxiliary_loss=auxiliary_loss)\n",
    "        train_error_ratios.append(nb_train_errors/train_input.size(0))\n",
    "        test_error_ratios.append(nb_test_errors/test_input.size(0))\n",
    "\n",
    "        if print_accuracy:\n",
    "            print('--- Run', run, '---')\n",
    "            print('- train error ratio:', nb_train_errors/train_input.size(0))\n",
    "            print('- test error ratio:', nb_test_errors/test_input.size(0))\n",
    "        if print_times: print('- time for training:', str(endTime-startTime))\n",
    "    if print_accuracy: \n",
    "        print('----END----')\n",
    "        print('Train error mean ratio:', np.mean(train_error_ratios))\n",
    "        print('Test errors mean ratio:', np.mean(test_error_ratios))\n",
    "\n",
    "    return train_error_ratios, test_error_ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import split\n",
    "\n",
    "def tune_model(model_name, number_of_rounds=5):\n",
    "    auxiliary_loss=False\n",
    "    if model_name == 'ConvNet':\n",
    "        model = ConvNet()\n",
    "    elif model_name == 'WeightSharing':\n",
    "        model = WeightSharing()\n",
    "    elif model_name == 'WeightSharingWithAuxiliaryLoss':\n",
    "        auxiliary_loss=True\n",
    "        model = WeightSharing(auxiliary_loss)\n",
    "    else:\n",
    "        raise ValueError('Please use one of the implemented methods: ConvNet, WeightSharing, WeightSharingWithAuxiliaryLoss')\n",
    "    \n",
    "    lrs = [5e-2, 1e-1, 5e-1, 1]\n",
    "    alphas = [0.2, 0.5, 0.8, 1]\n",
    "    gammas = [0.2, 0.5, 0.8, 1]\n",
    "    bestLr = 0\n",
    "    bestAlpha = 0\n",
    "    bestGamma = 0\n",
    "    best_percentage_validation_errors = 1\n",
    "    for lr in lrs:\n",
    "        if model_name == 'WeightSharingWithAuxiliaryLoss':\n",
    "            for alpha in alphas:\n",
    "                for gamma in gammas:\n",
    "                    percentage_validation_errors = []\n",
    "                    for _ in range(number_of_rounds):\n",
    "                        input, target, classes, _, _, _ = prologue.generate_pair_sets(number_of_pairs)\n",
    "                        \n",
    "\n",
    "                        train_input, validation_input = split(input, [int(number_of_pairs*0.5), int(number_of_pairs*0.5)])\n",
    "                        train_target, validation_target = split(target, [int(number_of_pairs*0.5), int(number_of_pairs*0.5)])\n",
    "                        train_classes, validation_classes = split(classes, [int(number_of_pairs*0.5), int(number_of_pairs*0.5)])\n",
    "\n",
    "                        model.train(True)\n",
    "                        train_model(model, train_input, train_target, train_classes, mini_batch_size, lr = lr, alpha = alpha, gamma = gamma, auxiliary_loss=auxiliary_loss)\n",
    "\n",
    "                        model.train(False)\n",
    "                        nb_validation_errors = compute_nb_errors(model, validation_input, validation_target, validation_classes, mini_batch_size, auxiliary_loss=auxiliary_loss)\n",
    "                        percentage_validation_errors.append(nb_validation_errors/len(validation_input))\n",
    "\n",
    "                    if np.mean(percentage_validation_errors) < best_percentage_validation_errors:\n",
    "                        best_percentage_validation_errors = np.mean(percentage_validation_errors)\n",
    "                        bestLr = lr\n",
    "                        bestAlpha = alpha\n",
    "                        bestGamma = gamma\n",
    "                        \n",
    "        else:\n",
    "            percentage_validation_errors = []\n",
    "            for round_number in range(number_of_rounds):\n",
    "                input, target, classes, _, _, _ = prologue.generate_pair_sets(number_of_pairs)\n",
    "\n",
    "                train_input, validation_input = split(input, [int(number_of_pairs*0.5), int(number_of_pairs*0.5)])\n",
    "                train_target, validation_target = split(target, [int(number_of_pairs*0.5), int(number_of_pairs*0.5)])\n",
    "                train_classes, validation_classes = split(classes, [int(number_of_pairs*0.5), int(number_of_pairs*0.5)])\n",
    "\n",
    "                model.train(True)\n",
    "                train_model(model, train_input, train_target, train_classes, mini_batch_size)\n",
    "\n",
    "                model.train(False)\n",
    "                nb_validation_errors = compute_nb_errors(model, validation_input, validation_target, validation_classes, mini_batch_size)\n",
    "                percentage_validation_errors.append(nb_validation_errors/len(validation_input))\n",
    "            \n",
    "            if np.mean(percentage_validation_errors) < best_percentage_validation_errors:\n",
    "                best_percentage_validation_errors = np.mean(percentage_validation_errors)\n",
    "                bestLr = lr\n",
    "\n",
    "    print('--- Best parameters ---')\n",
    "    print('lr:', bestLr, ', alpha:', bestAlpha, ', gamma:',bestGamma)\n",
    "    print('Mean percentage error on validation set :', best_percentage_validation_errors)\n",
    "    \n",
    "    return bestLr, bestAlpha, bestGamma\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Best parameters ---\n",
      "lr: 1 , alpha: 0 , gamma: 0\n",
      "Mean percentage error on validation set : 0.1272\n",
      "--- Run 0 ---\n",
      "- train error ratio: 0.009\n",
      "- test error ratio: 0.185\n",
      "--- Run 1 ---\n",
      "- train error ratio: 0.006\n",
      "- test error ratio: 0.185\n",
      "--- Run 2 ---\n",
      "- train error ratio: 0.003\n",
      "- test error ratio: 0.161\n",
      "--- Run 3 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.15\n",
      "--- Run 4 ---\n",
      "- train error ratio: 0.001\n",
      "- test error ratio: 0.183\n",
      "--- Run 5 ---\n",
      "- train error ratio: 0.026\n",
      "- test error ratio: 0.193\n",
      "--- Run 6 ---\n",
      "- train error ratio: 0.002\n",
      "- test error ratio: 0.184\n",
      "--- Run 7 ---\n",
      "- train error ratio: 0.009\n",
      "- test error ratio: 0.156\n",
      "--- Run 8 ---\n",
      "- train error ratio: 0.0\n",
      "- test error ratio: 0.177\n",
      "--- Run 9 ---\n",
      "- train error ratio: 0.009\n",
      "- test error ratio: 0.187\n",
      "----END----\n",
      "Train error mean ratio: 0.006599999999999999\n",
      "Test errors mean ratio: 0.1761\n"
     ]
    }
   ],
   "source": [
    "train_error_ratios, test_error_ratios = test_model('ConvNet', print_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Best parameters ---\n",
      "lr: 0.5 , alpha: 0 , gamma: 0\n",
      "Mean percentage error on validation set : 0.064\n",
      "--- Run 0 ---\n",
      "- train error ratio: 0.017\n",
      "- test error ratio: 0.137\n",
      "--- Run 1 ---\n",
      "- train error ratio: 0.01\n",
      "- test error ratio: 0.125\n",
      "--- Run 2 ---\n",
      "- train error ratio: 0.015\n",
      "- test error ratio: 0.156\n",
      "--- Run 3 ---\n",
      "- train error ratio: 0.006\n",
      "- test error ratio: 0.115\n",
      "--- Run 4 ---\n",
      "- train error ratio: 0.012\n",
      "- test error ratio: 0.134\n",
      "--- Run 5 ---\n",
      "- train error ratio: 0.008\n",
      "- test error ratio: 0.114\n",
      "--- Run 6 ---\n",
      "- train error ratio: 0.006\n",
      "- test error ratio: 0.113\n",
      "--- Run 7 ---\n",
      "- train error ratio: 0.006\n",
      "- test error ratio: 0.102\n",
      "--- Run 8 ---\n",
      "- train error ratio: 0.006\n",
      "- test error ratio: 0.105\n",
      "--- Run 9 ---\n",
      "- train error ratio: 0.005\n",
      "- test error ratio: 0.112\n",
      "----END----\n",
      "Train error mean ratio: 0.0091\n",
      "Test errors mean ratio: 0.1213\n"
     ]
    }
   ],
   "source": [
    "train_error_ratios, test_error_ratios = test_model('WeightSharing', print_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Weight Sharing and Auxiliary Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Best parameters ---\n",
      "lr: 0.1 , alpha: 1 , gamma: 0.8\n",
      "Mean percentage error on validation set : 0.009600000000000001\n",
      "--- Run 0 ---\n",
      "- train error ratio: 0.03\n",
      "- test error ratio: 0.185\n",
      "--- Run 1 ---\n",
      "- train error ratio: 0.038\n",
      "- test error ratio: 0.155\n",
      "--- Run 2 ---\n",
      "- train error ratio: 0.024\n",
      "- test error ratio: 0.125\n",
      "--- Run 3 ---\n",
      "- train error ratio: 0.036\n",
      "- test error ratio: 0.144\n",
      "--- Run 4 ---\n",
      "- train error ratio: 0.018\n",
      "- test error ratio: 0.165\n",
      "--- Run 5 ---\n",
      "- train error ratio: 0.021\n",
      "- test error ratio: 0.18\n",
      "--- Run 6 ---\n",
      "- train error ratio: 0.03\n",
      "- test error ratio: 0.166\n",
      "--- Run 7 ---\n",
      "- train error ratio: 0.026\n",
      "- test error ratio: 0.161\n",
      "--- Run 8 ---\n",
      "- train error ratio: 0.024\n",
      "- test error ratio: 0.14\n",
      "--- Run 9 ---\n",
      "- train error ratio: 0.014\n",
      "- test error ratio: 0.191\n",
      "----END----\n",
      "Train error mean ratio: 0.0261\n",
      "Test errors mean ratio: 0.16119999999999998\n"
     ]
    }
   ],
   "source": [
    "train_error_ratios, test_error_ratios = test_model('WeightSharingWithAuxiliaryLoss', print_accuracy=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e885cd74143ad494932267455ab53278514454996393c47fe6c2589217b9edf3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
